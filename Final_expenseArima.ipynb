{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269dce5c-325a-4837-a7cd-224498e58d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_log_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from scipy.stats import norm\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96d33a5-1467-449e-bfc0-3c98a09e0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv('training.csv', parse_dates=['transaction_date'])\n",
    "    card_group = pd.read_csv('mcc_group_definition.csv')\n",
    "    transaction = pd.read_csv('transaction_types.csv')\n",
    "    \n",
    "    train = train.drop(['dataset_transaction','dataset_user'], axis=1)\n",
    "    # merge data\n",
    "    train_final = train.merge(card_group, on='mcc_group', how='left').drop('mcc_group', axis=1)\n",
    "\n",
    "    train_final = (train_final\n",
    "          .merge(transaction, left_on='transaction_type', right_on='type', suffixes=('_card', '_transaction'))\n",
    "          .drop(['type','transaction_type'], axis=1))\n",
    "\n",
    "    train_final.amount_n26_currency = np.log1p(train_final.amount_n26_currency)\n",
    "    # segregating out data\n",
    "    in_data, out_data= train_final[(mask:=train_final['direction'] == \"In\")], train_final[~mask]\n",
    "    out_data = out_data.drop(['direction','explanation_card','explanation_transaction','agent'], axis=1)\n",
    "    out_data = out_data.sort_values(['user_id','transaction_date']).reset_index(drop=True)\n",
    "\n",
    "    return out_data, train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1c80f0-9ced-4e68-aefd-9d74513456ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the last 2 functions\n",
    "\n",
    "def plot_bar(feature,data):\n",
    "    g = sns.catplot(x=feature, \n",
    "                    data=data,\n",
    "                    order = data[feature].value_counts().index,\n",
    "                    kind='count', \n",
    "                    height=7, \n",
    "                    aspect = 3, \n",
    "                    palette=\"Paired\", \n",
    "                    edgecolor=\".6\")\n",
    "    g.set_xticklabels(rotation=90)\n",
    "\n",
    "def weekly_trend(feature):\n",
    "    f = train_final.groupby([feature, pd.Grouper(key='transaction_date',freq='W')]).size().reset_index()\n",
    "    f['transaction_date'] = f['transaction_date'].dt.date\n",
    "    f.columns = [feature,'transaction_date','count']\n",
    "    sns.catplot(x=\"transaction_date\", \n",
    "            y = 'count',\n",
    "            hue = feature, \n",
    "            data=f, \n",
    "            kind='point', \n",
    "            height=7, \n",
    "            aspect= 1,\n",
    "            palette='Paired')\n",
    "    _ = plt.xticks(rotation=90)\n",
    "    \n",
    "    \n",
    "def currency_trend(feature):\n",
    "    f = train_final.groupby([*feature, pd.Grouper(key='transaction_date',freq='W')])['amount_n26_currency'].mean().reset_index()\n",
    "    f_std = train_final.groupby([*feature, pd.Grouper(key='transaction_date',freq='W')])['amount_n26_currency'].std().reset_index()\n",
    "    f['transaction_date'] = f['transaction_date'].dt.date\n",
    "    \n",
    "    f_std['transaction_date'] = f_std['transaction_date'].dt.date\n",
    "    f.columns = [*feature,'transaction_date','mean']\n",
    "    f_std.columns = [*feature,'transaction_date','std']\n",
    "    \n",
    "    f_final = f.merge(f_std)\n",
    "    hue = feature[0]\n",
    "    \n",
    "    # correlating features or maybe not! \n",
    "    if feature[1]:\n",
    "        g = sns.catplot(x=\"transaction_date\", \n",
    "                y = 'mean',\n",
    "                hue = feature[0],\n",
    "                col = feature[1],\n",
    "                data=f, \n",
    "                kind='point', \n",
    "                height = 7,\n",
    "                aspect = 1.5,\n",
    "                palette=sns.color_palette(\"bright\", 10)\n",
    "                )\n",
    "        g.set_xticklabels(rotation=90)\n",
    "    else:\n",
    "        g = sns.catplot(x=\"transaction_date\", \n",
    "        y = 'mean',\n",
    "        hue = feature[0],\n",
    "        col = feature[1],\n",
    "        data=f, \n",
    "        kind='point', \n",
    "        height = 7,\n",
    "        aspect = 1.5,\n",
    "        palette=sns.color_palette(\"bright\", 10)\n",
    "        )\n",
    "        g.set_xticklabels(rotation=90)\n",
    "        autocorrelation_plot(f['mean'])\n",
    "#     x = np.arange(len(f['mean']))\n",
    "#   plt.errorbar(x,f_final['mean'],yerr=f_final['std'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5800e2-945b-4686-9b83-ab36023d5a24",
   "metadata": {},
   "source": [
    "## The data of primary interest which is the amount_n26_currecy is clearly postively sckwed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042ab773-fea3-4444-9882-d2427401ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_engineering_datetime(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "\n",
    "    def fit(self, data, label=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, label = None):\n",
    "        data[f\"{'transaction'}_day_week\"] = data['transaction_date'].dt.dayofweek\n",
    "        data[f\"{'transaction'}_week\"] = data['transaction_date'].dt.isocalendar().week\n",
    "        data[f\"{'transaction'}_week_day\"] = data['transaction_date'].dt.day\n",
    "        self.names = data[[x for x in data.columns if '_week' in x]].columns.tolist()\n",
    "       \n",
    "        return data[(elem for elem in data.columns if '_week' in elem)].values\n",
    "    def feature_names(self):\n",
    "        return self.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f30da8-7fdd-4bbc-8cee-d6749c05f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train2.values\n",
    "# history = [X[i] for i in range(10)]\n",
    "# yhat = np.mean([history[i] for i in range(10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb8ab7-daa9-4962-9bd8-ed59c3c75a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e877515-7d4e-4ce0-8a3e-8fc08ed7a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_engineering(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "\n",
    "    def fit(self, data, label=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, label = None):\n",
    "        #lags= [1,5,7,14]\n",
    "        lags = [7]\n",
    "        for lag in lags:\n",
    "            print('in timestamp of lags')\n",
    "        # lag feature\n",
    "        #\n",
    "            data[f'prev_amount_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).fillna(0)\n",
    "#             # rolling average\n",
    "            # grouping by the user id and then calculating the mean. Makes sense\n",
    "            data[f'prev_amount_rolling_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).mean().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().mean().fillna(0)\n",
    "#             # exponential moving average\n",
    "            data[f'prev_amount_ewm_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).ewm(0.1).mean().fillna(0)\n",
    "\n",
    "            data[f'prev_amount_rolling_std_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).std().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_std_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().std().fillna(0)\n",
    "#             # exponential moving average\n",
    "            data[f'prev_amount_ewm_std_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).ewm(0.1).std().fillna(0)\n",
    "         \n",
    "            ##\n",
    "#             # rolling average\n",
    "            # grouping by the user id and then calculating the mean. Makes sense\n",
    "            data[f'prev_amount_rolling_min_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).min().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_min_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().min().fillna(0)\n",
    "#             # exponential moving average\n",
    "#             data[f'prev_amount_ewm_min_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).ewm(0.1).min().fillna(0)\n",
    "         \n",
    "        \n",
    "            # rolling average\n",
    "            # grouping by the user id and then calculating the mean. Makes sense\n",
    "            data[f'prev_amount_rolling_max_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).max().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_max_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().max().fillna(0)\n",
    "   \n",
    "        \n",
    "        self.names = data[[x for x in data.columns if 'prev_' in x]].columns.tolist()\n",
    "\n",
    "    \n",
    "        return data[[x for x in data.columns if 'prev_' in x]].values\n",
    "        \n",
    "\n",
    "    def feature_names(self):\n",
    "        return self.names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a31002-0892-4127-853d-7b47d20d052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation pipeline\n",
    "# It combines all sorts of features that are transformed and combine them\n",
    "# to generate an output\n",
    "def generate_pipeline(**kwargs):\n",
    "    \n",
    "\n",
    "    features = ColumnTransformer(\n",
    "        [('date_time_features',feature_engineering_datetime(), ['transaction_date']),\n",
    "         ('lag_features',feature_engineering(), ['user_id','amount_n26_currency'])\n",
    "        ],  \n",
    "        remainder='drop',\n",
    "        sparse_threshold=0.3,\n",
    "        n_jobs=None,\n",
    "        transformer_weights=None\n",
    "    )      \n",
    "  \n",
    "    feature_transformer = Pipeline([\n",
    "        ('features', features),\n",
    "    ]) \n",
    "    return feature_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ee40f-6362-430b-93bd-2b6f9ea561dd",
   "metadata": {},
   "source": [
    "## we can deal with this by log transforming the series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50826878-4eec-4f78-a561-e332addbe86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # log_amont = np.log(train2.amount_n26_currency)\n",
    "# train2.amount_n26_currency = np.log(train2.amount_n26_currency)\n",
    "# # test the skew\n",
    "# # log_amont.skew() \n",
    "# plt.hist(train2.amount_n26_currency, bins=100,rwidth=0.8,density=True)\n",
    "# # plt.show()\n",
    "# rng = np.arange(train2.amount_n26_currency.min(), train2.amount_n26_currency.max(), 0.1)\n",
    "# plt.plot(rng,norm.pdf(rng,train2.amount_n26_currency.mean(),train2.amount_n26_currency.std()))\n",
    "# plt.axvline(train2.amount_n26_currency.mean(),ls=':',lw=2, color='b', label= 'mean = '+str(train2.amount_n26_currency.mean()))\n",
    "# plt.xlabel('amount_n26_currency')\n",
    "# plt.ylabel('')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9d801-5fa5-4364-82ef-8034bab1999e",
   "metadata": {},
   "source": [
    "## The log transformation has clearly showed some progress in terms of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449a8b98-cb14-4b2e-be11-e888fa5d2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add code for outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d1e43-a86c-4d76-b7f0-a613c5bab83f",
   "metadata": {},
   "source": [
    "# observing weekly trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c59af46-5691-42f5-aa76-a2bc1a0e3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the last 2 functions\n",
    "\n",
    "def plot_bar(feature,data):\n",
    "    g = sns.catplot(x=feature, \n",
    "                    data=data,\n",
    "                    order = data[feature].value_counts().index,\n",
    "                    kind='count', \n",
    "                    height=7, \n",
    "                    aspect = 3, \n",
    "                    palette=\"Paired\", \n",
    "                    edgecolor=\".6\")\n",
    "    g.set_xticklabels(rotation=90)\n",
    "\n",
    "def weekly_popularity_trend(feature):\n",
    "    \"\"\"\n",
    "    Feature popularity on weekly basis\n",
    "    \"\"\"\n",
    "    f = train_final.groupby([feature, pd.Grouper(key='transaction_date',freq='W')]).size().reset_index()\n",
    "    f['transaction_date'] = f['transaction_date'].dt.date\n",
    "    f.columns = [feature,'transaction_date','count']\n",
    "    sns.catplot(x=\"transaction_date\", \n",
    "            y = 'count',\n",
    "            hue = feature, \n",
    "            data=f, \n",
    "            kind='point', \n",
    "            height=7, \n",
    "            aspect= 1,\n",
    "            palette='Paired')\n",
    "    _ = plt.xticks(rotation=90)\n",
    "    \n",
    "    \n",
    "def weekly_currency_trend(feature):\n",
    "    \"\"\"\n",
    "    Mean transaction amount on a weekly basis for a given feature\n",
    "    \"\"\"\n",
    "#    weekly spending trend to get an idea about \n",
    "    f = train_final.groupby([*feature, pd.Grouper(key='transaction_date',freq='W')])['amount_n26_currency'].mean().reset_index()\n",
    "    f_std = train_final.groupby([*feature, pd.Grouper(key='transaction_date',freq='W')])['amount_n26_currency'].std().reset_index()\n",
    "    f['transaction_date'] = f['transaction_date'].dt.date\n",
    "    \n",
    "    f_std['transaction_date'] = f_std['transaction_date'].dt.date\n",
    "    f.columns = [*feature,'transaction_date','mean']\n",
    "    f_std.columns = [*feature,'transaction_date','std']\n",
    "    \n",
    "    f_final = f.merge(f_std)\n",
    "    hue = feature[0]\n",
    "    \n",
    "    # correlating features or maybe not! \n",
    "    if feature[1]:\n",
    "        g = sns.catplot(x=\"transaction_date\", \n",
    "                y = 'mean',\n",
    "                hue = feature[0],\n",
    "                col = feature[1],\n",
    "                data=f, \n",
    "                kind='point', \n",
    "                height = 7,\n",
    "                aspect = 1.5,\n",
    "                palette=sns.color_palette(\"bright\", 10)\n",
    "                )\n",
    "        g.set_xticklabels(rotation=90)\n",
    "    else:\n",
    "        g = sns.catplot(x=\"transaction_date\", \n",
    "        y = 'mean',\n",
    "        hue = feature[0],\n",
    "        col = feature[1],\n",
    "        data=f, \n",
    "        kind='point', \n",
    "        height = 7,\n",
    "        aspect = 1.5,\n",
    "        palette=sns.color_palette(\"bright\", 10)\n",
    "        )\n",
    "        g.set_xticklabels(rotation=90)\n",
    "        autocorrelation_plot(f['mean'])\n",
    "#     x = np.arange(len(f['mean']))\n",
    "#   plt.errorbar(x,f_final['mean'],yerr=f_final['std'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2aa6ab-feae-472c-9fc6-a58bde768c8d",
   "metadata": {},
   "source": [
    "# At the end of each month major income occures as should be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f6ea4-1b9c-4f28-be30-7b434e202c5a",
   "metadata": {},
   "source": [
    "## Feature Engineering date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "303309fb-e7ef-4981-a1d8-d4df69a4014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_engineering_datetime(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "\n",
    "    def fit(self, data, label=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, label = None):\n",
    "        data[f\"{'transaction'}_weekday\"] = data['transaction_date'].dt.dayofweek\n",
    "        data[f\"{'transaction'}_week\"] = data['transaction_date'].dt.isocalendar().week\n",
    "        data[f\"{'transaction'}_day_week\"] = data['transaction_date'].dt.day\n",
    "#         data[f\"{'transaction'}_week_year\"] = data['transaction_date'].dt.year\n",
    "        self.names = data[[x for x in data.columns if 'prev_' in x]].columns.tolist()\n",
    "       \n",
    "        return data[(elem for elem in data.columns if '_week' in elem)].values\n",
    "    def feature_names(self):\n",
    "        return self.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a588f31a-0b8b-4d1d-9dfd-e1c32e6986de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_engineering(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "\n",
    "    def fit(self, data, label=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, label = None):\n",
    "        #lags= [1,5,7,14]\n",
    "        lags = [7]\n",
    "        for lag in lags:\n",
    "            print('in timestamp of lags')\n",
    "        # lag feature\n",
    "        #A moving average is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles.\n",
    "            data[f'prev_amount_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).fillna(0)\n",
    "#             # rolling average\n",
    "            # grouping by the user id and then calculating the mean. Makes sense\n",
    "            data[f'prev_amount_rolling_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).mean().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().mean().fillna(0)\n",
    "#             # exponential moving average\n",
    "            data[f'prev_amount_ewm_mean_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).ewm(0.1).mean().fillna(0)\n",
    "         \n",
    "#             # rolling average\n",
    "            # grouping by the user id and then calculating the mean. Makes sense\n",
    "            data[f'prev_amount_rolling_std_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).std().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_std_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().std().fillna(0)\n",
    "#             # exponential moving average\n",
    "            data[f'prev_amount_ewm_std_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).ewm(0.1).std().fillna(0)\n",
    "         \n",
    "            ##\n",
    "#             # rolling average\n",
    "            # grouping by the user id and then calculating the mean. Makes sense\n",
    "            data[f'prev_amount_rolling_min_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).min().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_min_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().min().fillna(0)\n",
    "#             # exponential moving average\n",
    "#             data[f'prev_amount_ewm_min_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).ewm(0.1).min().fillna(0)\n",
    "         \n",
    "        \n",
    "        #             # rolling average\n",
    "            # grouping by the user id and then calculating the mean. Makes sense\n",
    "            data[f'prev_amount_rolling_max_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).rolling(2).max().fillna(0)\n",
    "#             # cumulative average\n",
    "            data[f'prev_amount_expand_max_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).expanding().max().fillna(0)\n",
    "#             # exponential moving average\n",
    "#             data[f'prev_amount_ewm_max_{lag}'] = data.groupby(['user_id'])['amount_n26_currency'].shift(lag).ewm(0.1).max().fillna(0)\n",
    "         \n",
    "        \n",
    "        self.names = data[[x for x in data.columns if 'prev_' in x]].columns.tolist()\n",
    "#         if label:\n",
    "#             y = df[label]\n",
    "#             return data[[x for x in data.columns if 'prev_' in x]].values, y.values\n",
    "    \n",
    "        return data[[x for x in data.columns if 'prev_' in x]].values\n",
    "        \n",
    "\n",
    "    def feature_names(self):\n",
    "        return self.names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb3323-8521-4b21-b34d-7669a582bed3",
   "metadata": {},
   "source": [
    "## pipeline creation for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "036b6fa0-8c5e-48fc-b877-d76945d1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation pipeline\n",
    "# It combines all sorts of features that are transformed and combine them\n",
    "# to generate an output\n",
    "def generate_pipeline(**kwargs):\n",
    "    \n",
    "\n",
    "    features = ColumnTransformer(\n",
    "        [('date_time_features',feature_engineering_datetime(), ['transaction_date']),\n",
    "         ('lag_features',feature_engineering(), ['user_id','amount_n26_currency'])\n",
    "        ],  \n",
    "        remainder='drop',\n",
    "        sparse_threshold=0.3,\n",
    "        n_jobs=None,\n",
    "        transformer_weights=None\n",
    "    )\n",
    "\n",
    "    \n",
    "#     # can use any sklearn estimator\n",
    "#     clf = XGBRegressor(n_estimators=100, random_state=42, booster='gbtree')\n",
    "        \n",
    "  \n",
    "    feature_transformer = Pipeline([\n",
    "        ('features', features),\n",
    "        # ('clf', clf)\n",
    "    ]) \n",
    "    return feature_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1c00034-6bb4-4092-8295-381b3f926d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metric(testy,predictions):\n",
    "    \n",
    "    # mean squared error\n",
    "    mse = mean_squared_error(testy, predictions)\n",
    "    print('mean squared error is: %f' %mse)\n",
    "    # mean forecast error\n",
    "    forecast_errors = [testy[i]-predictions[i] for i in range(len(testy))]\n",
    "    bias = sum(forecast_errors) * 1.0/len(testy)\n",
    "    print('Bias: %f' % bias)\n",
    "    \n",
    "    predictions_dataframe = pd.DataFrame({'y_true': np.exp(testy), 'y_pred': np.exp(predictions)})\n",
    "    plt.figure(figsize=(18,15))\n",
    "    plt.scatter(range(predictions_dataframe.shape[0]), predictions_dataframe['y_pred'], label='predicted', linestyle='dashed')\n",
    "    plt.scatter(range(predictions_dataframe.shape[0]), predictions_dataframe['y_true'], label='True', linestyle='dashed',alpha=0.3)\n",
    "    plt.ylabel('amount n26 currency')\n",
    "    plt.legend()\n",
    "    plt.savefig('expense_prediction.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f64378d-12f9-4218-8861-4fc5a201391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_forecast(train):\n",
    "    model_arima = sm.tsa.arima.ARIMA(train, order=(1,1,2))\n",
    "    model_fit = model_arima.fit()\n",
    "    yhat = model_fit.forecast()\n",
    "\n",
    "    return yhat,model_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3110cf93-1ef1-44d7-8ac0-a6de33176e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model(out_data,month):\n",
    "    train = out_data[out_data['transaction_date'].dt.month < month].reset_index(drop=True) # from feb to june\n",
    "    test = out_data[out_data['transaction_date'].dt.month == month].reset_index(drop=True) # test july\n",
    "\n",
    "    trainy = train['amount_n26_currency']\n",
    "    testy = test['amount_n26_currency']\n",
    "    \n",
    "    history = [x for x in train['amount_n26_currency']]\n",
    "    predictions,model_arima = arima_forecast(history)\n",
    "\n",
    "    return predictions,testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf754958-934c-4654-9ec7-6be36218b71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7292c7ac-e7f2-4d7c-9863-423ce4f8f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data, train_final = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b5c989e-70d8-433d-b700-d9ce504325c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gebruiker/.local/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "predictions,testy = single_model(out_data,month=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3fb503c7-51ac-4109-a94f-5bdd1f1e3c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.66776352])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643e956-565a-415b-9b46-538a752ad912",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric(testy,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a7e7cc6-b12c-459e-b06f-21ba3947cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_model(out_data):\n",
    "   \n",
    "    predictions = list()\n",
    "    # train till june and test for july\n",
    "    train = out_data[out_data['transaction_date'].dt.month < 7].reset_index(drop=True) # from feb to june\n",
    "    test = out_data[out_data['transaction_date'].dt.month == 7].reset_index(drop=True) # test july\n",
    "    history = [x for x in train['amount_n26_currency']]\n",
    "    for t in range(len(test)):\n",
    "        model = sm.tsa.arima.ARIMA(history, order=(1,1,2))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = test['amount_n26_currency'][t]\n",
    "        history.append(obs)\n",
    "        print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    # plot forecasts against actual outcomes\n",
    "    pyplot.plot(test)\n",
    "    pyplot.plot(predictions, color='red')\n",
    "    pyplot.show()\n",
    "    return predictions\n",
    "#     print(model_fit.summary())\n",
    "#     residuals = pd.DataFrame(model_fit.resid)\n",
    "#     plt.figure(figsize=(15,15))\n",
    "#     fig, ax = plt.subplots(1,2)\n",
    "#     residuals.plot(title=\"Residuals\", ax=ax[0])\n",
    "#     residuals.plot(kind='kde', title='Density', ax=ax[1])\n",
    "#     forecast_arima = model_fit.forecast()\n",
    "#     plt.show()\n",
    "#     return forecast_arima\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284c7c2-78dd-4dd8-a5e6-5c96acdbddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gebruiker/.local/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted=2.667764, expected=2.302585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gebruiker/.local/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted=2.640285, expected=2.079442\n",
      "predicted=2.589339, expected=2.772589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gebruiker/.local/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted=2.627002, expected=4.442651\n"
     ]
    }
   ],
   "source": [
    "preds = arima_model(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900482f-015d-4d7a-a4c4-afb74bfaedc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c784a-906d-45f3-a362-39963aae78b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ad5c9-4ec5-46a7-aba7-2d97f2c6bb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7df39e42-34ed-493e-84f1-35e6ffea2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(out_data):\n",
    "    predictions = list()\n",
    "    \n",
    "    # change to 8 for august!\n",
    "    train = out_data[out_data['transaction_date'].dt.month <= 7].reset_index(drop=True) # from feb to june\n",
    "    test = out_data[out_data['transaction_date'].dt.month == 7].reset_index(drop=True) # test july\n",
    "    model = generate_pipeline()\n",
    "    model.fit(train)\n",
    "    # should the data be transformed after each loop?\n",
    "    \n",
    "    history = [x for x in train['amount_n26_currency']]\n",
    "    trainY = np.log1p(train['amount_n26_currency'])\n",
    "    testY = np.log1p(test['amount_n26_currency'])\n",
    "    history = transformed_train\n",
    "    for i in range(len(transformed_test)):\n",
    "        model = sm.tsa.arima.ARIMA(history, order=(1,1,2))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = test['amount_n26_currency'][t]\n",
    "        history.append(obs)\n",
    "        print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    # plot forecasts against actual outcomes\n",
    "    pyplot.plot(test)\n",
    "    pyplot.plot(predictions, color='red')\n",
    "    pyplot.show()\n",
    "    return predictions,testY\n",
    "        \n",
    "        \n",
    "        \n",
    "#         test, testy = transformed_test[i],testY[i]\n",
    "#         test = test.reshape(1,len(test))\n",
    "#         yhat = arima_forecast(history ,trainY,test)\n",
    "#         predictions.append(yhat)\n",
    "# #         history.append(test)\n",
    "#         history = np.vstack( [ history , transformed_test[i]] )\n",
    "#         trainY = trainY.append(pd.Series(testY[i]),ignore_index= True)\n",
    "#         print(testy, yhat)\n",
    "        \n",
    "    return predictions,testy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbd71754-b565-4aa5-8f06-23fce6bfca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in timestamp of lags\n",
      "in timestamp of lags\n",
      "in timestamp of lags\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "SARIMAX models require univariate `endog`. Got shape (336421, 14).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b3e62cdf2fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwalk_forward_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-0a74014d222f>\u001b[0m in \u001b[0;36mwalk_forward_validation\u001b[0;34m(out_data)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARIMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mmodel_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/statsmodels/tsa/arima/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, order, seasonal_order, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# this criteria. Instead, we'll just make sure that the parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# estimates from those methods satisfy the criteria.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         self._spec_arima = SARIMAXSpecification(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseasonal_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseasonal_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mtrend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_stationarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_invertibility\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/statsmodels/tsa/arima/specification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, order, seasonal_order, ar_order, diff, ma_order, seasonal_ar_order, seasonal_diff, seasonal_ma_order, seasonal_periods, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[1;32m    452\u001b[0m         if (validate_specification and not faux_endog and\n\u001b[1;32m    453\u001b[0m                 self.endog.ndim > 1 and self.endog.shape[1] > 1):\n\u001b[0;32m--> 454\u001b[0;31m             raise ValueError('SARIMAX models require univariate `endog`. Got'\n\u001b[0m\u001b[1;32m    455\u001b[0m                              ' shape %s.' % str(self.endog.shape))\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: SARIMAX models require univariate `endog`. Got shape (336421, 14)."
     ]
    }
   ],
   "source": [
    "walk_forward_validation(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39e15c-e234-4f7a-bd32-eebea35984c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
